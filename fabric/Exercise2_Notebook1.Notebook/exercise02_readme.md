# Exercise 02 â€“ Umsatzanalyse mit PySpark & Visualisierung (Microsoft Fabric)

## ğŸ¯ Ziel der Ãœbung
Ziel dieser Ãœbung ist es, die in Exercise 01 erstellten Lakehouse-Daten mit **PySpark**
weiterzuverarbeiten und die Ergebnisse mithilfe von **Python-Visualisierungen**
anschaulich darzustellen.

Der Fokus liegt auf:
- Arbeiten mit PySpark DataFrames in Microsoft Fabric
- Aggregationen und Transformationen auf Spark-Ebene
- Ãœbergang von verteiltem Compute (Spark) zu lokaler Visualisierung
- Erstellung einfacher Diagramme mit seaborn
- VerstÃ¤ndnis des Zusammenspiels von Spark, pandas und Visualisierung

---

## ğŸ“¦ Datengrundlage
- **Quelle:** `sales.csv`
- **Speicherort:** Microsoft Fabric Lakehouse
- **Tabelle:** `sales`
- **SchlÃ¼sselspalten:**
  - `Item`
  - `Quantity`
  - `UnitPrice`

Die Daten wurden bereits in Exercise 01 in das Lakehouse geladen.

---

## ğŸ› ï¸ Verwendete Technologien
- Microsoft Fabric Notebook
- Apache Spark (PySpark)
- pandas
- seaborn
- matplotlib

---

## ğŸ§® Analyseschritte

### 1ï¸âƒ£ Laden der Lakehouse-Tabelle als Spark DataFrame
```python
df = spark.read.table("sales")
```

---

### 2ï¸âƒ£ Berechnung des Umsatzes pro Produkt
```python
from pyspark.sql.functions import col, sum as _sum

revenue_df = (
    df.withColumn("Revenue", col("Quantity") * col("UnitPrice"))
      .groupBy("Item")
      .agg(_sum("Revenue").alias("TotalRevenue"))
      .orderBy(col("TotalRevenue").desc())
)
```

---

### 3ï¸âƒ£ Konvertierung in pandas DataFrame
```python
pdf = revenue_df.toPandas()
```

âš ï¸ Hinweis:  
Dieser Schritt ist nur fÃ¼r **kleine bis mittlere Ergebnisdatenmengen** geeignet.
Die eigentliche Berechnung erfolgt weiterhin vollstÃ¤ndig auf Spark-Ebene.

---

## ğŸ“Š Visualisierung der Ergebnisse

### 4ï¸âƒ£ Balkendiagramm: Umsatz pro Produkt
```python
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
sns.barplot(
    data=pdf,
    x="TotalRevenue",
    y="Item",
    palette="viridis"
)

plt.title("Umsatz pro Produkt")
plt.xlabel("Gesamtumsatz")
plt.ylabel("Produkt")
plt.tight_layout()
plt.show()
```

---

## ğŸ“Œ Erkenntnisse
- Spark eignet sich hervorragend fÃ¼r skalierbare Datenverarbeitung
- FÃ¼r Visualisierungen ist hÃ¤ufig eine Konvertierung nach pandas sinnvoll
- Microsoft Fabric ermÃ¶glicht einen nahtlosen Ãœbergang zwischen
  verteiltem Compute und interaktiver Analyse
- Die Kombination aus Spark + Python + seaborn ist ein typischer
  Data-Engineering- und Analytics-Workflow

---

## ğŸ”— Einordnung im Lernpfad
Diese Ãœbung baut direkt auf **Exercise 01** auf und erweitert sie um:
- praktische PySpark-Nutzung
- VerstÃ¤ndnis fÃ¼r DatenflÃ¼sse innerhalb von Fabric
- erste Visualisierungsschritte fÃ¼r Analyse & Reporting
- Vorbereitung auf Power BI und weiterfÃ¼hrende Analytics-Szenarien

Sie ist ein zentraler Baustein fÃ¼r die **DP-600 â€“ Microsoft Fabric Analytics Engineer** Zertifizierung.
